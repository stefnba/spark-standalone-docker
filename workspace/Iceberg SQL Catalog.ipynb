{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fddf100b-89c2-4976-ab84-29ee3ceb71ac",
   "metadata": {},
   "source": [
    "# Iceberg with SQL Catalog\n",
    "\n",
    "Example of Iceberg with SQL Postgres Catalog and Minio as S3 Compatible Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a91fbd38-9f10-4cbe-bf8c-45b021de588a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/22 13:37:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PySpark 3.5.0 version is running...\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"spark://spark-master:7077\")  # type: ignore\n",
    "    .appName(\"Testing Iceberg with SQL Catalog\")\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(f'The PySpark {spark.version} version is running...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dcfffd-707a-4797-a0de-af621f1e63b2",
   "metadata": {},
   "source": [
    "### Test Spark is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cad235f9-b039-4549-ae84-9d3859cf1100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "| John| 28|\n",
      "|Alice| 35|\n",
      "|  Bob| 42|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"John\", 28), (\"Alice\", 35), (\"Bob\", 42)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "df.createOrReplaceTempView(\"tmp_people\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d79f667-64ca-470e-a798-01aece532a7a",
   "metadata": {},
   "source": [
    "## Config Iceberg catalog\n",
    "\n",
    "```bash\n",
    "spark-sql \\\n",
    "    --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \\\n",
    "    --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog \\\n",
    "    --conf spark.sql.catalog.spark_catalog.catalog-impl=org.apache.iceberg.jdbc.JdbcCatalog \\\n",
    "    --conf spark.sql.catalog.spark_catalog.jdbc.user=iceberg \\\n",
    "    --conf spark.sql.catalog.spark_catalog.jdbc.password=iceberg \\\n",
    "    --conf spark.sql.catalog.spark_catalog.uri=jdbc:postgresql://catalog:5432/iceberg \\\n",
    "    --conf spark.sql.catalog.spark_catalog.io-impl=org.apache.iceberg.aws.s3.S3FileIO \\\n",
    "    --conf spark.sql.catalog.spark_catalog.s3.endpoint=http://minio:9000 \\\n",
    "    --conf spark.sql.catalog.spark_catalog.warehouse=s3://warehouse/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b6a1c47-f965-4fa8-9d0d-1718ea2eb956",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG_NAME = \"demo\"\n",
    "DEFAULT_NAMESPACE = \"default\"\n",
    "\n",
    "spark.conf.set(f\"spark.sql.catalog.{CATALOG_NAME}\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "spark.conf.set(f\"spark.sql.catalog.{CATALOG_NAME}.catalog-impl\", \"org.apache.iceberg.jdbc.JdbcCatalog\")\n",
    "spark.conf.set(f\"spark.sql.catalog.{CATALOG_NAME}.default-namespace\", DEFAULT_NAMESPACE)\n",
    "spark.conf.set(f\"spark.sql.catalog.{CATALOG_NAME}.uri\", \"jdbc:postgresql://catalog:5432/iceberg\")\n",
    "spark.conf.set(f\"spark.sql.catalog.{CATALOG_NAME}.jdbc.user\", \"iceberg\")\n",
    "spark.conf.set(f\"spark.sql.catalog.{CATALOG_NAME}.jdbc.password\", \"iceberg\")\n",
    "spark.conf.set(f\"spark.sql.catalog.{CATALOG_NAME}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "spark.conf.set(f\"spark.sql.catalog.{CATALOG_NAME}.s3.endpoint\", \"http://minio:9000\")\n",
    "spark.conf.set(f\"spark.sql.catalog.{CATALOG_NAME}.warehouse\", \"s3://warehouse/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768da27c-9c96-4e22-b8dc-a83b26b295a7",
   "metadata": {},
   "source": [
    "### Change Catalog\n",
    "\n",
    "Activate the catalog and optionally the namespace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa5c1c66-f54d-404e-809c-52e7230f2d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "USE demo;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60585de9-e856-4c58-9336-8eb52390c028",
   "metadata": {},
   "source": [
    "### Create new Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "176322bf-64de-4bc3-998a-534f0ba98012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "CREATE NAMESPACE new_namespace;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a472cb21-ece7-4762-83df-b6989c4d0ce7",
   "metadata": {},
   "source": [
    "### Activate namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "396201d9-1168-46cc-a015-74cf53b43022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "USE demo.new_namespace;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f02bb9",
   "metadata": {},
   "source": [
    "### List existing catalogs, namespaces, tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e384ee6-e295-411b-9000-15fc1cc18180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>catalog</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>demo</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>spark_catalog</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+---------------+\n",
       "|       catalog |\n",
       "+---------------+\n",
       "|          demo |\n",
       "| spark_catalog |\n",
       "+---------------+"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SHOW catalogs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8347596d-54a9-44bd-8966-b9f5d0a986e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>namespace</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>new</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>new_namespace</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+---------------+\n",
       "|     namespace |\n",
       "+---------------+\n",
       "|           new |\n",
       "| new_namespace |\n",
       "+---------------+"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SHOW namespaces;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3759ed17-6584-4e5e-a2f2-11bde4a33609",
   "metadata": {},
   "source": [
    "### Create table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58eb05b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "CREATE TABLE IF NOT EXISTS people (\n",
    "  name STRING,\n",
    "  age INT\n",
    ") USING iceberg;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7934cda-e3a9-4373-8a21-4f9d9c913692",
   "metadata": {},
   "source": [
    "Create a table in a different Namespace than the one activated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e6f9702-1a2e-4680-8494-188265a443a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "CREATE TABLE IF NOT EXISTS default.quote (date Date, Close Float) USING iceberg PARTITIONED BY (YEAR(date));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ad8d88-5def-4740-8e93-c09376214e5e",
   "metadata": {},
   "source": [
    "### Show tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f296309b-dc32-48fb-8d53-aa320ed3ede0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>namespace</th>\n",
       "            <th>tableName</th>\n",
       "            <th>isTemporary</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>new_namespace</td>\n",
       "            <td>people</td>\n",
       "            <td>False</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+---------------+-----------+-------------+\n",
       "|     namespace | tableName | isTemporary |\n",
       "+---------------+-----------+-------------+\n",
       "| new_namespace |    people |       False |\n",
       "+---------------+-----------+-------------+"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SHOW tables;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d2a6a7-aede-4818-be5f-15aa610fd9d5",
   "metadata": {},
   "source": [
    "## Insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63967bcf-ddfd-4455-b65f-ac8bcdce9609",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "quote = spark.read.csv(\"data/GDAXI.INDX.csv\", header=True, inferSchema=True).select([\"Date\", \"Close\"])\n",
    "quote.writeTo(\"default.quote\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6f4f610-3c12-48ae-8c16-39580fa4fdc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/22 13:43:38 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "23/12/22 13:43:38 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:981)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 37924)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/local/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/local/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/local/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/spark/python/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/spark/python/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "-- Insert from temp view\n",
    "INSERT INTO people SELECT * FROM tmp_people"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2c6dd8-67fc-4ab4-a759-713f44e65e84",
   "metadata": {},
   "source": [
    "## Query Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d98c682b-d241-4a29-84b2-66e8b62b6102",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o30.sql.\n: org.apache.iceberg.exceptions.NotFoundException: Location does not exist: s3://warehouse/new_namespace/people/metadata/00005-b17537b3-620f-40f1-8ec2-3ca4f561d5bc.metadata.json\n\tat org.apache.iceberg.aws.s3.S3InputStream.openStream(S3InputStream.java:194)\n\tat org.apache.iceberg.aws.s3.S3InputStream.positionStream(S3InputStream.java:177)\n\tat org.apache.iceberg.aws.s3.S3InputStream.read(S3InputStream.java:107)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.ensureLoaded(ByteSourceJsonBootstrapper.java:539)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.detectEncoding(ByteSourceJsonBootstrapper.java:133)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.constructParser(ByteSourceJsonBootstrapper.java:256)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.JsonFactory._createParser(JsonFactory.java:1744)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.JsonFactory.createParser(JsonFactory.java:1143)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3809)\n\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:273)\n\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:266)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$0(BaseMetastoreTableOperations.java:189)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$1(BaseMetastoreTableOperations.java:208)\n\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:208)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:185)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:176)\n\tat org.apache.iceberg.jdbc.JdbcTableOperations.doRefresh(JdbcTableOperations.java:100)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refresh(BaseMetastoreTableOperations.java:97)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.current(BaseMetastoreTableOperations.java:80)\n\tat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:47)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\n\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1908)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\n\tat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:166)\n\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:643)\n\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:159)\n\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:355)\n\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:336)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$3(Analyzer.scala:1268)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1267)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1259)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1123)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1087)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1214)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1087)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1046)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:226)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:222)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:173)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:209)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3, Status Code: 404, Request ID: 17A32AF2745A81AC, Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handleErrorResponse(CombinedResponseHandler.java:125)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handleResponse(CombinedResponseHandler.java:82)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handle(CombinedResponseHandler.java:60)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handle(CombinedResponseHandler.java:41)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:30)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:72)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:42)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:78)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:40)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:52)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:37)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:81)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:36)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:56)\n\tat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:36)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:80)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:50)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:32)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)\n\tat software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:196)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:171)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$0(BaseSyncClientHandler.java:68)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:179)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:62)\n\tat software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:52)\n\tat software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:63)\n\tat software.amazon.awssdk.services.s3.DefaultS3Client.getObject(DefaultS3Client.java:4483)\n\tat org.apache.iceberg.aws.s3.S3InputStream.openStream(S3InputStream.java:192)\n\t... 107 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msql\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSELECT * FROM people;\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2517\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2515\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2516\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2517\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2519\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2520\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/.ipython/profile_default/startup/00-prettytables.py:81\u001b[0m, in \u001b[0;36msql\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _to_table(df, num_rows\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlimit)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _to_table(\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o30.sql.\n: org.apache.iceberg.exceptions.NotFoundException: Location does not exist: s3://warehouse/new_namespace/people/metadata/00005-b17537b3-620f-40f1-8ec2-3ca4f561d5bc.metadata.json\n\tat org.apache.iceberg.aws.s3.S3InputStream.openStream(S3InputStream.java:194)\n\tat org.apache.iceberg.aws.s3.S3InputStream.positionStream(S3InputStream.java:177)\n\tat org.apache.iceberg.aws.s3.S3InputStream.read(S3InputStream.java:107)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.ensureLoaded(ByteSourceJsonBootstrapper.java:539)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.detectEncoding(ByteSourceJsonBootstrapper.java:133)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.constructParser(ByteSourceJsonBootstrapper.java:256)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.JsonFactory._createParser(JsonFactory.java:1744)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.JsonFactory.createParser(JsonFactory.java:1143)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3809)\n\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:273)\n\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:266)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$0(BaseMetastoreTableOperations.java:189)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$1(BaseMetastoreTableOperations.java:208)\n\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:208)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:185)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:176)\n\tat org.apache.iceberg.jdbc.JdbcTableOperations.doRefresh(JdbcTableOperations.java:100)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refresh(BaseMetastoreTableOperations.java:97)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.current(BaseMetastoreTableOperations.java:80)\n\tat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:47)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\n\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1908)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\n\tat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:166)\n\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:643)\n\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:159)\n\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:355)\n\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:336)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$3(Analyzer.scala:1268)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1267)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1259)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1123)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1087)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1214)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1087)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1046)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:226)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:222)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:173)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:209)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3, Status Code: 404, Request ID: 17A32AF2745A81AC, Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handleErrorResponse(CombinedResponseHandler.java:125)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handleResponse(CombinedResponseHandler.java:82)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handle(CombinedResponseHandler.java:60)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handle(CombinedResponseHandler.java:41)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:30)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:72)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:42)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:78)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:40)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:52)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:37)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:81)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:36)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:56)\n\tat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:36)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:80)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:50)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:32)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)\n\tat software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:196)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:171)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$0(BaseSyncClientHandler.java:68)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:179)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:62)\n\tat software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:52)\n\tat software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:63)\n\tat software.amazon.awssdk.services.s3.DefaultS3Client.getObject(DefaultS3Client.java:4483)\n\tat org.apache.iceberg.aws.s3.S3InputStream.openStream(S3InputStream.java:192)\n\t... 107 more\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT * FROM people;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "aba5f751-178d-411f-aa1f-bf898e77ed90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>date</th>\n",
       "            <th>Close</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>2010-01-04</td>\n",
       "            <td>6048.2998046875</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-01-05</td>\n",
       "            <td>6031.85986328125</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-01-06</td>\n",
       "            <td>6034.330078125</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-01-07</td>\n",
       "            <td>6019.35986328125</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-01-08</td>\n",
       "            <td>6037.60986328125</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-01-11</td>\n",
       "            <td>6040.5</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-01-12</td>\n",
       "            <td>5943.0</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-01-13</td>\n",
       "            <td>5963.14013671875</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-01-14</td>\n",
       "            <td>5988.8798828125</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-01-15</td>\n",
       "            <td>5875.97021484375</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-01-18</td>\n",
       "            <td>5918.5498046875</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-01-19</td>\n",
       "            <td>5976.47998046875</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-01-20</td>\n",
       "            <td>5851.52978515625</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-01-21</td>\n",
       "            <td>5746.97021484375</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-01-22</td>\n",
       "            <td>5695.31982421875</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-01-25</td>\n",
       "            <td>5631.3701171875</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-01-26</td>\n",
       "            <td>5668.93017578125</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-01-27</td>\n",
       "            <td>5643.2001953125</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-01-28</td>\n",
       "            <td>5540.330078125</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-01-29</td>\n",
       "            <td>5608.7900390625</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-02-01</td>\n",
       "            <td>5654.47998046875</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-02-02</td>\n",
       "            <td>5709.66015625</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-02-03</td>\n",
       "            <td>5672.08984375</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-02-04</td>\n",
       "            <td>5533.240234375</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-02-05</td>\n",
       "            <td>5434.33984375</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-02-08</td>\n",
       "            <td>5484.85009765625</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-02-09</td>\n",
       "            <td>5498.259765625</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-02-10</td>\n",
       "            <td>5536.3701171875</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-02-11</td>\n",
       "            <td>5503.93017578125</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-02-12</td>\n",
       "            <td>5500.39013671875</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-02-15</td>\n",
       "            <td>5511.10009765625</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-02-16</td>\n",
       "            <td>5592.1201171875</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-02-17</td>\n",
       "            <td>5648.33984375</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-02-18</td>\n",
       "            <td>5680.41015625</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-02-19</td>\n",
       "            <td>5722.0498046875</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-02-22</td>\n",
       "            <td>5688.43994140625</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-02-23</td>\n",
       "            <td>5604.06982421875</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-02-24</td>\n",
       "            <td>5615.509765625</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-02-25</td>\n",
       "            <td>5532.330078125</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-02-26</td>\n",
       "            <td>5598.4599609375</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-03-01</td>\n",
       "            <td>5713.509765625</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-03-02</td>\n",
       "            <td>5776.56005859375</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-03-03</td>\n",
       "            <td>5817.8798828125</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-03-04</td>\n",
       "            <td>5795.31982421875</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-03-05</td>\n",
       "            <td>5877.35986328125</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-03-08</td>\n",
       "            <td>5875.91015625</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-03-09</td>\n",
       "            <td>5885.89013671875</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-03-10</td>\n",
       "            <td>5936.72021484375</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-03-11</td>\n",
       "            <td>5928.6298828125</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-03-12</td>\n",
       "            <td>5945.10986328125</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-03-15</td>\n",
       "            <td>5903.56005859375</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-03-16</td>\n",
       "            <td>5970.990234375</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-03-17</td>\n",
       "            <td>6024.27978515625</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-03-18</td>\n",
       "            <td>6012.31005859375</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-03-19</td>\n",
       "            <td>5982.43017578125</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-03-22</td>\n",
       "            <td>5987.5</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-03-23</td>\n",
       "            <td>6017.27001953125</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-03-24</td>\n",
       "            <td>6039.0</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-03-25</td>\n",
       "            <td>6132.9501953125</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-03-26</td>\n",
       "            <td>6120.0498046875</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-03-29</td>\n",
       "            <td>6156.85009765625</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-03-30</td>\n",
       "            <td>6142.4501953125</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-03-31</td>\n",
       "            <td>6153.5498046875</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-04-01</td>\n",
       "            <td>6235.56005859375</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-04-06</td>\n",
       "            <td>6252.2099609375</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-04-07</td>\n",
       "            <td>6222.41015625</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-04-08</td>\n",
       "            <td>6171.830078125</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-04-09</td>\n",
       "            <td>6249.7001953125</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-04-12</td>\n",
       "            <td>6250.68994140625</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-04-13</td>\n",
       "            <td>6230.830078125</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-04-14</td>\n",
       "            <td>6278.39990234375</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-04-15</td>\n",
       "            <td>6291.4501953125</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-04-16</td>\n",
       "            <td>6180.89990234375</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-04-19</td>\n",
       "            <td>6162.43994140625</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-04-20</td>\n",
       "            <td>6264.22998046875</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-04-21</td>\n",
       "            <td>6230.3798828125</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-04-22</td>\n",
       "            <td>6168.72021484375</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-04-23</td>\n",
       "            <td>6259.52978515625</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-04-26</td>\n",
       "            <td>6332.10009765625</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-04-27</td>\n",
       "            <td>6159.509765625</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-04-28</td>\n",
       "            <td>6084.33984375</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-04-29</td>\n",
       "            <td>6144.91015625</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-04-30</td>\n",
       "            <td>6135.7001953125</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-05-03</td>\n",
       "            <td>6166.919921875</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-05-04</td>\n",
       "            <td>6006.85986328125</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-05-05</td>\n",
       "            <td>5958.4501953125</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-05-06</td>\n",
       "            <td>5908.259765625</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-05-07</td>\n",
       "            <td>5715.08984375</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-05-10</td>\n",
       "            <td>6017.91015625</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-05-11</td>\n",
       "            <td>6037.7099609375</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-05-12</td>\n",
       "            <td>6183.490234375</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-05-13</td>\n",
       "            <td>6251.97021484375</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-05-14</td>\n",
       "            <td>6056.7099609375</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-05-17</td>\n",
       "            <td>6066.919921875</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-05-18</td>\n",
       "            <td>6155.93017578125</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-05-19</td>\n",
       "            <td>5988.669921875</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-05-20</td>\n",
       "            <td>5867.8798828125</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-05-21</td>\n",
       "            <td>5829.25</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-05-24</td>\n",
       "            <td>5805.68017578125</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2010-05-25</td>\n",
       "            <td>5670.0400390625</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+------------+------------------+\n",
       "|       date |            Close |\n",
       "+------------+------------------+\n",
       "| 2010-01-04 |  6048.2998046875 |\n",
       "| 2010-01-05 | 6031.85986328125 |\n",
       "| 2010-01-06 |   6034.330078125 |\n",
       "| 2010-01-07 | 6019.35986328125 |\n",
       "| 2010-01-08 | 6037.60986328125 |\n",
       "| 2010-01-11 |           6040.5 |\n",
       "| 2010-01-12 |           5943.0 |\n",
       "| 2010-01-13 | 5963.14013671875 |\n",
       "| 2010-01-14 |  5988.8798828125 |\n",
       "| 2010-01-15 | 5875.97021484375 |\n",
       "| 2010-01-18 |  5918.5498046875 |\n",
       "| 2010-01-19 | 5976.47998046875 |\n",
       "| 2010-01-20 | 5851.52978515625 |\n",
       "| 2010-01-21 | 5746.97021484375 |\n",
       "| 2010-01-22 | 5695.31982421875 |\n",
       "| 2010-01-25 |  5631.3701171875 |\n",
       "| 2010-01-26 | 5668.93017578125 |\n",
       "| 2010-01-27 |  5643.2001953125 |\n",
       "| 2010-01-28 |   5540.330078125 |\n",
       "| 2010-01-29 |  5608.7900390625 |\n",
       "| 2010-02-01 | 5654.47998046875 |\n",
       "| 2010-02-02 |    5709.66015625 |\n",
       "| 2010-02-03 |    5672.08984375 |\n",
       "| 2010-02-04 |   5533.240234375 |\n",
       "| 2010-02-05 |    5434.33984375 |\n",
       "| 2010-02-08 | 5484.85009765625 |\n",
       "| 2010-02-09 |   5498.259765625 |\n",
       "| 2010-02-10 |  5536.3701171875 |\n",
       "| 2010-02-11 | 5503.93017578125 |\n",
       "| 2010-02-12 | 5500.39013671875 |\n",
       "| 2010-02-15 | 5511.10009765625 |\n",
       "| 2010-02-16 |  5592.1201171875 |\n",
       "| 2010-02-17 |    5648.33984375 |\n",
       "| 2010-02-18 |    5680.41015625 |\n",
       "| 2010-02-19 |  5722.0498046875 |\n",
       "| 2010-02-22 | 5688.43994140625 |\n",
       "| 2010-02-23 | 5604.06982421875 |\n",
       "| 2010-02-24 |   5615.509765625 |\n",
       "| 2010-02-25 |   5532.330078125 |\n",
       "| 2010-02-26 |  5598.4599609375 |\n",
       "| 2010-03-01 |   5713.509765625 |\n",
       "| 2010-03-02 | 5776.56005859375 |\n",
       "| 2010-03-03 |  5817.8798828125 |\n",
       "| 2010-03-04 | 5795.31982421875 |\n",
       "| 2010-03-05 | 5877.35986328125 |\n",
       "| 2010-03-08 |    5875.91015625 |\n",
       "| 2010-03-09 | 5885.89013671875 |\n",
       "| 2010-03-10 | 5936.72021484375 |\n",
       "| 2010-03-11 |  5928.6298828125 |\n",
       "| 2010-03-12 | 5945.10986328125 |\n",
       "| 2010-03-15 | 5903.56005859375 |\n",
       "| 2010-03-16 |   5970.990234375 |\n",
       "| 2010-03-17 | 6024.27978515625 |\n",
       "| 2010-03-18 | 6012.31005859375 |\n",
       "| 2010-03-19 | 5982.43017578125 |\n",
       "| 2010-03-22 |           5987.5 |\n",
       "| 2010-03-23 | 6017.27001953125 |\n",
       "| 2010-03-24 |           6039.0 |\n",
       "| 2010-03-25 |  6132.9501953125 |\n",
       "| 2010-03-26 |  6120.0498046875 |\n",
       "| 2010-03-29 | 6156.85009765625 |\n",
       "| 2010-03-30 |  6142.4501953125 |\n",
       "| 2010-03-31 |  6153.5498046875 |\n",
       "| 2010-04-01 | 6235.56005859375 |\n",
       "| 2010-04-06 |  6252.2099609375 |\n",
       "| 2010-04-07 |    6222.41015625 |\n",
       "| 2010-04-08 |   6171.830078125 |\n",
       "| 2010-04-09 |  6249.7001953125 |\n",
       "| 2010-04-12 | 6250.68994140625 |\n",
       "| 2010-04-13 |   6230.830078125 |\n",
       "| 2010-04-14 | 6278.39990234375 |\n",
       "| 2010-04-15 |  6291.4501953125 |\n",
       "| 2010-04-16 | 6180.89990234375 |\n",
       "| 2010-04-19 | 6162.43994140625 |\n",
       "| 2010-04-20 | 6264.22998046875 |\n",
       "| 2010-04-21 |  6230.3798828125 |\n",
       "| 2010-04-22 | 6168.72021484375 |\n",
       "| 2010-04-23 | 6259.52978515625 |\n",
       "| 2010-04-26 | 6332.10009765625 |\n",
       "| 2010-04-27 |   6159.509765625 |\n",
       "| 2010-04-28 |    6084.33984375 |\n",
       "| 2010-04-29 |    6144.91015625 |\n",
       "| 2010-04-30 |  6135.7001953125 |\n",
       "| 2010-05-03 |   6166.919921875 |\n",
       "| 2010-05-04 | 6006.85986328125 |\n",
       "| 2010-05-05 |  5958.4501953125 |\n",
       "| 2010-05-06 |   5908.259765625 |\n",
       "| 2010-05-07 |    5715.08984375 |\n",
       "| 2010-05-10 |    6017.91015625 |\n",
       "| 2010-05-11 |  6037.7099609375 |\n",
       "| 2010-05-12 |   6183.490234375 |\n",
       "| 2010-05-13 | 6251.97021484375 |\n",
       "| 2010-05-14 |  6056.7099609375 |\n",
       "| 2010-05-17 |   6066.919921875 |\n",
       "| 2010-05-18 | 6155.93017578125 |\n",
       "| 2010-05-19 |   5988.669921875 |\n",
       "| 2010-05-20 |  5867.8798828125 |\n",
       "| 2010-05-21 |          5829.25 |\n",
       "| 2010-05-24 | 5805.68017578125 |\n",
       "| 2010-05-25 |  5670.0400390625 |\n",
       "+------------+------------------+"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT * FROM quote;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37338dad-6e1a-4df4-8e0c-db1d04c7e2da",
   "metadata": {},
   "source": [
    "### Stop Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f001a1c6-588f-4026-84b0-247564cea3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
